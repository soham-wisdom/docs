---
title: "Evaluation Tab"
---

The Evaluation tab is used to test and validate AI behavior within a domain. Within this section, you can create specific sets of prompts to simulate user interactions. Provide expected SQL outputs alongside these prompts to directly measure the AI's accuracy in translating natural language into correct database queries. After running evaluations, you can review the results to identify areas for improvement.

<Frame>
  ![UI snippet showing the Evaluation Tab](/images/new-ui-evaluation-tab.png)
</Frame>

### **Evaluation Sets**

Create and manage collections of prompts for evaluating AI behavior. Add a new evaluation set by providing a name and a JSON definition containing prompts, optionally including expected SQL. Existing evaluation sets are listed in this view, allowing you to review and manage them over time.

### **Evaluation Runs**

Find a list of completed evaluation runs and their results. This view allows you to review past executions, compare outcomes, and track how AI performance changes over time.

<Note>
  To learn how to create an Evaluation Set, understand indicators, and review results, consult the related article [Use Evaluation Sets and Runs](/improve-wisdom-ai-responses/use-evaluations-sets-and-runs).
</Note>
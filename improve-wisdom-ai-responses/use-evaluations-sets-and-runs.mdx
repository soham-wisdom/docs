---
title: "Use Evaluations Sets and Runs"
---

**Evaluation Sets** and **Evaluation Runs** are crucial for rigorously testing and validating the performance of your domain's AI, ensuring it consistently generates accurate and relevant responses.

## Evaluation Sets

Evaluation Sets allow you to **create specific sets of prompts** that simulate user interactions. A key feature is the ability to **provide expected SQL outputs** alongside these prompts. This direct comparison helps measure the AI's accuracy in translating natural language into correct database queries.

To create an Evaluation Set:

1. Open a Domain and click on the **Evaluation** tab.
2. Navigate to the **Evaluation Sets** Sub-tab.
3. Click **Add Evaluation**.

![Image showing the Evaluations tab](/images/add-evaluation.png)

4. Fill out the **Add Evaluation** Form, providing a set **Name** and an array of prompts in JSON format, optionally including the expected SQL.

   <Tip>
     The array you provide defines how many conversations will be created. Each _top-level element_ in the array represents a conversation:

     ✔ If it's a string or an object → You get a single-message conversation\
     ✔ If it's an array → You create a multi-message conversation

     For example, this input:\
     `["prompt1", ["prompt2.1", "prompt2.2"], "prompt3"]`\
     creates **3 conversations**: the first and third with one message each, and the second with two messages.

     You can structure the arrays in any way, mixing single and multi-message conversations as required.
   </Tip>
5. Click on **Save**. The new Evaluation set will appear listed.

![Image showing the Add Evaluations form](/images/add-evaluation-form.png)

## Evaluation Runs

Evaluation Runs are where the AI processes your defined Evaluation Sets. After running these evaluations, you can review the results to identify areas for improvement.

To Run an Evaluation, go to the Evaluations Set tab, select an evaluation and click on **Run**. You will see the results in the Evaluation Runs tab.

![Run Evaluation Pn](/images/run-evaluation.png)

### Evaluation Runs Indicators

**Status:** Signals its progress or completion.

- Running: The evaluation run is currently in progress.
- Completed: The evaluation run has finished successfully.

**Score:** Reflects the result of the completed evaluation. It tells you how many conversations passed based on the predefined **evaluation criteria** (i.e. Prompt \+ Expected SQL result added in the Evaluation set modal).

<Tip>
  The Score evaluation criteria refers to how much the generated answer matches the expected SQL result defined previously.
</Tip>

![Evaluation Runs Indicatorsv2 Pn](/images/evaluation-runs-indicatorsv2.png)

### Evaluation Report

When you click on the **View Report** option, you will see comprehensive details about how an evaluation run performed. Here's a breakdown of the information you'll find:

- **View Domain:** A link that allows you to navigate to the specific domain that was evaluated.
- **Soft Match:** This **Score** indicates the overall performance of the evaluation. It shows how many of the evaluation criteria were successfully met out of the total. It is named as Soft Match since it is possible that results are consider a match even if they are not exactly the same as the expected (provided) SQL.
- **Individual Session Details**: The report organizes the evaluation results by individual sessions or queries (e.g., Session 1, Session 2).

![View Report 01v2 Gi](/images/view-report-01v2.gif)

**Session Details**

For each session, you can explore the following:

- **Session Title:** This clearly states the query or task that was evaluated for that particular session (e.g., "Calculate total revenue for closed won opportunities using ACV.").
- **Show Evaluation Details:** This section, which you can expand, provides specific insights into how the session's evaluation was conducted and its outcome:
  - **Prompt:** The exact prompt that was used for this session.
  - **Manual Score:** You have the ability to manually score the evaluation (✅ ❌). This overrides the Automated Score calculation.
  - **Automated Score:** The score automatically assigned by the system for this evaluation. It corresponds to the Soft Match.
  - **Ground Truth:** This is the expected or correct outcome of the query. It often includes details about the relevant domain tables (e.g., SELECT SUM(amount) FROM Opportunity), helping you understand why the system's output might differ from the expected result.
  - **Generated Result:** This displays the output produced by the system, often including the underlying SQL query that was generated and the final numerical result (e.g., "\$137.55M").

![Session Details Gi](/images/session-details.gif)

<Warning>
  Providing an incorrect or inaccurate SQL code will lead to a syntax or semantic error, and the system will provide a message in red detailing the nature of the error, often including the location within the query where the problem occurred, to aid in correction.

  ![Incorrect Sql Example Pn](/images/incorrect-sql-example.png)
</Warning>

- **Conversation:** You can expand this section to review the entire conversational exchange related to that specific session. This includes:
  - **AI Workstream:** This part offers a deeper look into the AI's processing of your query. It typically includes:
    - **Tool Selected:** The specific tool or method the AI used (e.g., "Tabular Data").
    - **Selected Examples:** Relevant examples of similar queries and their corresponding SQL, which guided the AI's understanding.
    - **Step-by-step plan:** A detailed breakdown of the logical steps the AI followed to interpret your query, identify the necessary data, and construct the solution.
  - A **Reviewed** status, which confirms whether the response has been reviewed and often provides a summary of the outcome (e.g., "The user received a complete response to their request with a specific value for total revenue from closed won opportunities using ACV, so no further action or information is needed.").

![Conversation Detailed Gi](/images/conversation-detailed.gif)

Navigating the report helps you understand the system's processing, verify generated results, and review the underlying conversational logic.

## Next Steps

- [Configure Domains](/improve-wisdom-ai-responses/configure-domains)
- [Curate Knowledge Workflow](/improve-wisdom-ai-responses/curate-knowledge-workflow)
- [Basic Tutorial: Connect and Test](/setting-up-wisdom-ai/basic-tutorial-connect-and-test)
- [Advanced Tutorial: Fine Tuning Your Context](/setting-up-wisdom-ai/advanced-tutorial-fine-tuning-your-context)
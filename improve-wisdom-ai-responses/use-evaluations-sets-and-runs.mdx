---
title: "Use Evaluations Sets and Runs"
description: "Learn how to effectively use Evaluation Sets and Evaluation Runs to test and validate your AI's performance, ensuring accurate and relevant responses."
---

**Evaluation Sets** and **Evaluation Runs** are crucial for rigorously testing and validating the performance of your domain's AI, ensuring it consistently generates accurate and relevant responses.

## Evaluation Sets

**Evaluation Sets** allow you to create specific sets of prompts that simulate user interactions. A key feature is the ability to provide **expected SQL outputs** alongside these prompts. This direct comparison helps measure the AI's accuracy in translating natural language into correct database queries.

To create an Evaluation Set:

- Navigate to the **Evaluation Sets Sub-tab**.
- Click **Add Evaluation**.
- Fill out the **Add Evaluation Form**, providing a set name and a JSON input for prompts, optionally including the expected SQL.

The **Evaluation Sets List** displays all your existing evaluation sets.

![Image showing the Evaluation Sets sub-tab](/images/eva-sets.png)

## Evaluation Runs

**Evaluation Runs** are where the AI processes your defined Evaluation Sets. After running these evaluations, you can review the results to identify areas for improvement.

The **Evaluation Runs List** (found in the **Evaluation Runs Sub-tab**) displays all completed evaluation runs and their corresponding results.

![Image showing the Evaluation Runs sub-tab](/images/eva-runs.png)